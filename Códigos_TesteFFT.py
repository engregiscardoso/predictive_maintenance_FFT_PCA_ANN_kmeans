from scipy.io import loadmat
import pandas as pd
import numpy as np
import tensorflow as tf
from keras.models import load_model

from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
import numpy as np
import joblib
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

import seaborn as sns
from sklearn.metrics import confusion_matrix
import serial
import time
import os

import scipy.io

import math #For identifying NaNs(Not a Number)
import pickle #For compressing files
import glob #For dealing with file paths and directories
import os #For dealing with file paths and directories
import tqdm #For making progress bars
import datetime #For dealing with time in general
from sklearn.cluster import KMeans
from collections import Counter

# -*- coding: utf-8 -*-
"""
Created on Mon May  8 09:41:52 2023

@author: REGIS CARDOSO
"""

# -*- coding: utf-8 -*-
"""ICPHM23_1500rpm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YoTKoqsuRw0j1HFAt8AMbT8KPLCS0qdH
"""
### DECLARATION FROM LIBRARIES

import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import pandas as pd
from pandas.plotting import lag_plot
from pandas.plotting import autocorrelation_plot
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import scipy
import scipy.stats as stats
from scipy.signal import filtfilt
from scipy.fft import fft, fftfreq
from pylab import rcParams
from mpl_toolkits import mplot3d



### ORGANIZE THE DATASET WITHOUT THE TIME COLUMN

def ajust_df_sem_time(df_base):
  value = []
  
  df_base = df_base.reset_index()
  del df_base['index']
  
  df_lines = len(df_base)
  
  for i in range(df_lines):
        value.append(df_base.loc[i])
        
  df_ts = []
  df_ts = pd.DataFrame()

  df_ts['value'] = pd.concat(value)
  df_ts = df_ts.reset_index()
  
  

  return df_ts['value']


### CONCATENATES THE DATA BY NAMING THE COLUMNS MAINTAINING THE SAME PATTERN

def ajust_df_x_concat(df_base, numero_linhas):
    # Inicializar DataFrame vazio
    df_concat_100_linhas_0 = pd.DataFrame()
    
    # Calcular número de blocos possíveis
    n_blocos = len(df_base) // numero_linhas
    
    # Loop para criar colunas com blocos
    for i in range(n_blocos):
        # Nome da coluna
        coluna = f'Dado_x15_{i}_0'
        
        # Definir os índices do bloco
        start_idx = i * numero_linhas
        end_idx = (i + 1) * numero_linhas  # Ajuste para índices inclusivos no slicing
        
        # Fatiar e ajustar os dados
        bloco = df_base.iloc[start_idx:end_idx].reset_index(drop=True)
        
        # Verificar se o bloco tem dados e ajustar
        if not bloco.empty:
            # Ajustar o bloco e adicioná-lo ao DataFrame final
            df_concat_100_linhas_0[coluna] = ajust_df_sem_time(bloco)
        else:
            print(f"Bloco vazio em {coluna}, ignorado.")
    
    return df_concat_100_linhas_0


### CREATE THE TIME COLUMN IN THE DATASET

def creat_time_colun(df_base):
    df = df_base
    df['time'] = 0
    for i in range(len(df)):
      df['time'].loc[i] = i*(1/10000)
      
    return df['time']


### FUNCTION TO PERFORM THE FFT OF THE SIGNAL BASED ON X AND Y

def FFT_x_y(graf_y, graf_x):

    x = graf_x
    y = graf_y
    N = len(graf_x)

    T = x[1] - x[0]
    Fs = 1 / T
    
    yf = 2.0 / N * np.abs(fft(y)[0:N // 2])

    xf = fftfreq(N, T)[:N // 2]

    verX = []
    verY = []

    obs = len(yf)

    for i in range(1, obs, 1):
        verX.insert(i, xf[i])
        verY.insert(i, yf[i])
        
    df_FFT = []
    df_FFT = pd.DataFrame(df_FFT)

    df_FFT['F'] = verX
    df_FFT['A'] = verY

    return (df_FFT)


### PREPARE THE DATASET USING FREQUENCY DOMAIN BY FFT

def prepare_transpost_df_x(df_base, fault_code):
    
    df_FFT = []
    df_FFT = pd.DataFrame(df_FFT)
    
    cols = df_base.shape[1]
    
    for i in range(cols-1):
        coluna = 'Dado_x15_'+str(i)+'_0'
        df_concat_parcial = FFT_x_y(df_base[coluna].values, df_base['time'].values)
        
        coluna = 'Dado_x15_'+str(i)+'_'+ str(fault_code)
        
        df_FFT[coluna] = df_concat_parcial['A']
      
    return df_FFT


### READING, ORGANIZING AND RETURN OF DATASETs

def read_and_separe_dataset():
    
    # every 200 values is equivalent to 20ms
    path = ''
    #Time domain vibration signals under operational condition of 1500-rpm motor speed and 10Nm load
    x_1500_10_file = path + 'x_1500_10.npy' #x direction
    y_1500_10_file = path + 'y_1500_10.npy' #y direction
    z_1500_10_file = path + 'z_1500_10.npy' #z direction
    
    #ground-truth labels for four different types of faults
    gt_1500_10_file = path + 'gt_1500_10.npy'
    
    #load files 
    x_1500_10 = np.load(x_1500_10_file)
    y_1500_10 = np.load(y_1500_10_file)
    z_1500_10 = np.load(z_1500_10_file)
    gt_1500_10 = np.load(gt_1500_10_file)
    
    #Convert numpy datasets into pandas datasets
    dfx15 = pd.DataFrame(x_1500_10)
    dfy15 = pd.DataFrame(y_1500_10)
    dfz15 = pd.DataFrame(z_1500_10)
    dfgt15 = pd.DataFrame(gt_1500_10)
    
    dfx15['fault_code'] = dfgt15[0]
    dfy15['fault_code'] = dfgt15[0]
    dfz15['fault_code'] = dfgt15[0]
    
    dfx15_0 = dfx15[dfx15['fault_code']==0] # Fault Code ==0
    dfx15_1 = dfx15[dfx15['fault_code']==1] # Fault Code ==1
    dfx15_2 = dfx15[dfx15['fault_code']==2] # Fault Code ==2
    dfx15_3 = dfx15[dfx15['fault_code']==3] # Fault Code ==3
    dfx15_4 = dfx15[dfx15['fault_code']==4] # Fault Code ==4
    dfy15_0 = dfy15[dfy15['fault_code']==0] # Fault Code ==0
    dfy15_1 = dfy15[dfy15['fault_code']==1] # Fault Code ==1
    dfy15_2 = dfy15[dfy15['fault_code']==2] # Fault Code ==2
    dfy15_3 = dfy15[dfy15['fault_code']==3] # Fault Code ==3
    dfy15_4 = dfy15[dfy15['fault_code']==4] # Fault Code ==4
    dfz15_0 = dfz15[dfz15['fault_code']==0] # Fault Code ==0
    dfz15_1 = dfz15[dfz15['fault_code']==1] # Fault Code ==1
    dfz15_2 = dfz15[dfz15['fault_code']==2] # Fault Code ==2
    dfz15_3 = dfz15[dfz15['fault_code']==3] # Fault Code ==3
    dfz15_4 = dfz15[dfz15['fault_code']==4] # Fault Code ==4
    
    del dfx15_0['fault_code']
    del dfx15_1['fault_code']
    del dfx15_2['fault_code']
    del dfx15_3['fault_code']
    del dfx15_4['fault_code']
    del dfy15_0['fault_code']
    del dfy15_1['fault_code']
    del dfy15_2['fault_code']
    del dfy15_3['fault_code']
    del dfy15_4['fault_code']
    del dfz15_0['fault_code']
    del dfz15_1['fault_code']
    del dfz15_2['fault_code']
    del dfz15_3['fault_code']
    del dfz15_4['fault_code']
    
    dfx15_0 = dfx15_0.reset_index()
    dfx15_0 = dfx15_0.drop(columns=['index'])
    dfx15_1 = dfx15_1.reset_index()
    dfx15_1 = dfx15_1.drop(columns=['index'])
    dfx15_2 = dfx15_2.reset_index()
    dfx15_2 = dfx15_2.drop(columns=['index'])
    dfx15_3 = dfx15_3.reset_index()
    dfx15_3 = dfx15_3.drop(columns=['index'])
    dfx15_4 = dfx15_4.reset_index()
    dfx15_4 = dfx15_4.drop(columns=['index'])
    
    dfy15_0 = dfy15_0.reset_index()
    dfy15_0 = dfy15_0.drop(columns=['index'])
    dfy15_1 = dfy15_1.reset_index()
    dfy15_1 = dfy15_1.drop(columns=['index'])
    dfy15_2 = dfy15_2.reset_index()
    dfy15_2 = dfy15_2.drop(columns=['index'])
    dfy15_3 = dfy15_3.reset_index()
    dfy15_3 = dfy15_3.drop(columns=['index'])
    dfy15_4 = dfy15_4.reset_index()
    dfy15_4 = dfy15_4.drop(columns=['index'])
    
    dfz15_0 = dfz15_0.reset_index()
    dfz15_0 = dfz15_0.drop(columns=['index'])
    dfz15_1 = dfz15_1.reset_index()
    dfz15_1 = dfz15_1.drop(columns=['index'])
    dfz15_2 = dfz15_2.reset_index()
    dfz15_2 = dfz15_2.drop(columns=['index'])
    dfz15_3 = dfz15_3.reset_index()
    dfz15_3 = dfz15_3.drop(columns=['index'])
    dfz15_4 = dfz15_4.reset_index()
    dfz15_4 = dfz15_4.drop(columns=['index'])
    
    return dfx15_0, dfx15_1, dfx15_2, dfx15_3, dfx15_4, dfy15_0, dfy15_1, dfy15_2, dfy15_3, dfy15_4, dfz15_0, dfz15_1, dfz15_2, dfz15_3, dfz15_4


####################################################################################

### DATASET DESCRIPTION
#ICPHM2023 Data Challenge Dataset
#The dataset includes vibration signals of normal and four different types of fault conditions and their corresponding ground-truth labels for two different operational conditions.
#• Vibration signals have been divided into smaller segments using window size of 200 data points to reduce the computational time.
#• Vibration signals are available in three directions (x, y, z). Participants are free to use either one direction or any combination of them.
#Each vibration signal is recorded in three directions (x, y, z), for a period of five minutes, and with a sampling frequency of 10 kHz. The examples of time-domain signal acquired in
#different operating conditions.
#sample rate 10 kHz= 10.000 samples per second.


####################################################################################

## READING DOOS DATASETs
dfx15_0, dfx15_1, dfx15_2, dfx15_3, dfx15_4, dfy15_0, dfy15_1, dfy15_2, dfy15_3, dfy15_4, dfz15_0, dfz15_1, dfz15_2, dfz15_3, dfz15_4 = read_and_separe_dataset()


### ORGANIZES AND JOINS DATA BASED ON THE NUMBER OF ROWS DESIRED FOR ANALYSIS
### (1OO LINES IS A GOOD VALUE) WHICH RESULTS IN AN ACQUISITION OF 2 SECONDS OF DATA

### Use ajust_df_x_concat for the X axis so that the column names all have the same for concatenation

### To print the complete dataset use number_lines = 10000

### To print the training and testing dataset number_lines = 100

numero_linhas = 100

df_x_concat_100_linhas_0 = ajust_df_x_concat(dfx15_0, numero_linhas)
df_x_concat_100_linhas_1 = ajust_df_x_concat(dfx15_1, numero_linhas)
df_x_concat_100_linhas_2 = ajust_df_x_concat(dfx15_2, numero_linhas)
df_x_concat_100_linhas_3 = ajust_df_x_concat(dfx15_3, numero_linhas)
df_x_concat_100_linhas_4 = ajust_df_x_concat(dfx15_4, numero_linhas)

df_y_concat_100_linhas_0 = ajust_df_x_concat(dfy15_0, numero_linhas)
df_y_concat_100_linhas_1 = ajust_df_x_concat(dfy15_1, numero_linhas)
df_y_concat_100_linhas_2 = ajust_df_x_concat(dfy15_2, numero_linhas)
df_y_concat_100_linhas_3 = ajust_df_x_concat(dfy15_3, numero_linhas)
df_y_concat_100_linhas_4 = ajust_df_x_concat(dfy15_4, numero_linhas)

df_z_concat_100_linhas_0 = ajust_df_x_concat(dfz15_0, numero_linhas)
df_z_concat_100_linhas_1 = ajust_df_x_concat(dfz15_1, numero_linhas)
df_z_concat_100_linhas_2 = ajust_df_x_concat(dfz15_2, numero_linhas)
df_z_concat_100_linhas_3 = ajust_df_x_concat(dfz15_3, numero_linhas)
df_z_concat_100_linhas_4 = ajust_df_x_concat(dfz15_4, numero_linhas)



####################################################################################

### ADD THE TIME COLUMN TO THE DATASET, CREATE A DATASET WHERE COLUMNS ARE THE DATA AND THE LINES ARE THE TIME OF ACQUISITION OF EACH SIGNAL

df_x_concat_100_linhas_0['time'] = creat_time_colun(df_x_concat_100_linhas_0)
df_x_concat_100_linhas_1['time'] = creat_time_colun(df_x_concat_100_linhas_1)
df_x_concat_100_linhas_2['time'] = creat_time_colun(df_x_concat_100_linhas_2)
df_x_concat_100_linhas_3['time'] = creat_time_colun(df_x_concat_100_linhas_3)
df_x_concat_100_linhas_4['time'] = creat_time_colun(df_x_concat_100_linhas_4)

df_y_concat_100_linhas_0['time'] = creat_time_colun(df_y_concat_100_linhas_0)
df_y_concat_100_linhas_1['time'] = creat_time_colun(df_y_concat_100_linhas_1)
df_y_concat_100_linhas_2['time'] = creat_time_colun(df_y_concat_100_linhas_2)
df_y_concat_100_linhas_3['time'] = creat_time_colun(df_y_concat_100_linhas_3)
df_y_concat_100_linhas_4['time'] = creat_time_colun(df_y_concat_100_linhas_4)

df_z_concat_100_linhas_0['time'] = creat_time_colun(df_z_concat_100_linhas_0)
df_z_concat_100_linhas_1['time'] = creat_time_colun(df_z_concat_100_linhas_1)
df_z_concat_100_linhas_2['time'] = creat_time_colun(df_z_concat_100_linhas_2)
df_z_concat_100_linhas_3['time'] = creat_time_colun(df_z_concat_100_linhas_3)
df_z_concat_100_linhas_4['time'] = creat_time_colun(df_z_concat_100_linhas_4)



figx, axs = plt.subplots(5,1, figsize=(40,20))
axs[0].plot(df_x_concat_100_linhas_0['time'], df_x_concat_100_linhas_0['Dado_x15_0_0'])
#axs[0].set_xlim([0, 2])
axs[0].set_ylim([-0.5, 0.5])

axs[1].plot(df_x_concat_100_linhas_1['time'], df_x_concat_100_linhas_1['Dado_x15_0_0'])
#axs[1].set_xlim([0, 2])
axs[1].set_ylim([-0.5, 0.5])

axs[2].plot(df_x_concat_100_linhas_2['time'], df_x_concat_100_linhas_2['Dado_x15_0_0'])
#axs[2].set_xlim([0, 2])
axs[2].set_ylim([-0.5, 0.5])

axs[3].plot(df_x_concat_100_linhas_3['time'], df_x_concat_100_linhas_3['Dado_x15_0_0'])
#axs[3].set_xlim([0, 2])
axs[3].set_ylim([-0.5, 0.5])

axs[4].plot(df_x_concat_100_linhas_4['time'], df_x_concat_100_linhas_4['Dado_x15_0_0'])
#axs[4].set_xlim([0, 2])
axs[4].set_ylim([-0.5, 0.5])

axs[0].set(title='Original Dataset')
axs[0].set(ylabel='Label 0 \n\n')
axs[1].set(ylabel='Label 1 \n\n')
axs[2].set(ylabel='Label 2 \n\nValue')
axs[3].set(ylabel='Label 3 \n\n')
axs[4].set(xlabel='Time')
axs[4].set(ylabel='Label 4 \n\n')

axs[0].figure.set_size_inches(20, 10)

axs[0].axes.get_xaxis().set_visible(False)
axs[1].axes.get_xaxis().set_visible(False)
axs[2].axes.get_xaxis().set_visible(False)
axs[3].axes.get_xaxis().set_visible(False)


axs[0].grid()
axs[1].grid()
axs[2].grid()
axs[3].grid()
axs[4].grid()


string = r'images\timeplottotaly.png'
plt.savefig(string)
plt.show()
plt.cla()
plt.clf()
plt.close()




####################################################################################

### PLOT FOR VISUALIZING DATA IN THE TIME DOMAIN ONLY ONE LINE, PRESENTS ONLY ONE ACQUISITION OF EACH LABEL

figx, axs = plt.subplots(5,1, figsize=(40,20))
axs[0].plot(df_x_concat_100_linhas_0['time'].loc[0:200], df_x_concat_100_linhas_0['Dado_x15_0_0'].loc[0:200])
axs[0].set_xlim([0, 0.02])
axs[0].set_ylim([-0.5, 0.5])

axs[1].plot(df_x_concat_100_linhas_1['time'].loc[0:200], df_x_concat_100_linhas_1['Dado_x15_0_0'].loc[0:200])
axs[1].set_xlim([0, 0.02])
axs[1].set_ylim([-0.5, 0.5])

axs[2].plot(df_x_concat_100_linhas_2['time'].loc[0:200], df_x_concat_100_linhas_2['Dado_x15_0_0'].loc[0:200])
axs[2].set_xlim([0, 0.02])
axs[2].set_ylim([-0.5, 0.5])

axs[3].plot(df_x_concat_100_linhas_3['time'].loc[0:200], df_x_concat_100_linhas_3['Dado_x15_0_0'].loc[0:200])
axs[3].set_xlim([0, 0.02])
axs[3].set_ylim([-0.5, 0.5])

axs[4].plot(df_x_concat_100_linhas_4['time'].loc[0:200], df_x_concat_100_linhas_4['Dado_x15_0_0'].loc[0:200])
axs[4].set_xlim([0, 0.02])
axs[4].set_ylim([-0.5, 0.5])

axs[0].set(title='Original Dataset - Only a data line')
axs[0].set(ylabel='Label 0 \n\n')
axs[1].set(ylabel='Label 1 \n\n')
axs[2].set(ylabel='Label 2 \n\nValue')
axs[3].set(ylabel='Label 3 \n\n')
axs[4].set(xlabel='Time')
axs[4].set(ylabel='Label 4 \n\n')

axs[0].figure.set_size_inches(20, 10)

axs[0].axes.get_xaxis().set_visible(False)
axs[1].axes.get_xaxis().set_visible(False)
axs[2].axes.get_xaxis().set_visible(False)
axs[3].axes.get_xaxis().set_visible(False)


axs[0].grid()
axs[1].grid()
axs[2].grid()
axs[3].grid()
axs[4].grid()


string = r'images\timeplot1line.png'
plt.savefig(string)
plt.show()
plt.cla()
plt.clf()
plt.close()
    


### PLOT FOR VISUALIZING DATA IN THE TIME DOMAIN, PRESENTS ONLY ONE ACQUISITION OF EACH LABEL

figx, axs = plt.subplots(5,1, figsize=(40,20))
axs[0].plot(df_x_concat_100_linhas_0['time'], df_x_concat_100_linhas_0['Dado_x15_0_0'])
axs[0].set_xlim([0, 2])
axs[0].set_ylim([-0.5, 0.5])

axs[1].plot(df_x_concat_100_linhas_1['time'], df_x_concat_100_linhas_1['Dado_x15_0_0'])
axs[1].set_xlim([0, 2])
axs[1].set_ylim([-0.5, 0.5])

axs[2].plot(df_x_concat_100_linhas_2['time'], df_x_concat_100_linhas_2['Dado_x15_0_0'])
axs[2].set_xlim([0, 2])
axs[2].set_ylim([-0.5, 0.5])

axs[3].plot(df_x_concat_100_linhas_3['time'], df_x_concat_100_linhas_3['Dado_x15_0_0'])
axs[3].set_xlim([0, 2])
axs[3].set_ylim([-0.5, 0.5])

axs[4].plot(df_x_concat_100_linhas_4['time'], df_x_concat_100_linhas_4['Dado_x15_0_0'])
axs[4].set_xlim([0, 2])
axs[4].set_ylim([-0.5, 0.5])

axs[0].set(title='Time Domain - New Dataset')
axs[0].set(ylabel='Label 0 \n\n')
axs[1].set(ylabel='Label 1 \n\n')
axs[2].set(ylabel='Label 2 \n\nValue')
axs[3].set(ylabel='Label 3 \n\n')
axs[4].set(xlabel='Time')
axs[4].set(ylabel='Label 4 \n\n')

axs[0].figure.set_size_inches(20, 10)

axs[0].axes.get_xaxis().set_visible(False)
axs[1].axes.get_xaxis().set_visible(False)
axs[2].axes.get_xaxis().set_visible(False)
axs[3].axes.get_xaxis().set_visible(False)


axs[0].grid()
axs[1].grid()
axs[2].grid()
axs[3].grid()
axs[4].grid()


string = r'images\timeplot.png'
plt.savefig(string)
plt.show()
plt.cla()
plt.clf()
plt.close()





####################################################################################

### TRANSFORMS THE DATASET FROM THE TIME DOMAIN TO THE FREQUENCY DOMAIN, THROUGH THE APPLICATION OF FFT
### CREATES A DATASET WHERE THE COLUMNS ARE THE DATA AND THE ROWS THE FREQUENCY PRESENT IN THE SIGNALS

df_x_FFT_dfx15_0 = prepare_transpost_df_x(df_x_concat_100_linhas_0, 0)
df_x_FFT_dfx15_1 = prepare_transpost_df_x(df_x_concat_100_linhas_1, 1)
df_x_FFT_dfx15_2 = prepare_transpost_df_x(df_x_concat_100_linhas_2, 2)
df_x_FFT_dfx15_3 = prepare_transpost_df_x(df_x_concat_100_linhas_3, 3)
df_x_FFT_dfx15_4 = prepare_transpost_df_x(df_x_concat_100_linhas_4, 4)

df_y_FFT_dfx15_0 = prepare_transpost_df_x(df_y_concat_100_linhas_0, 0)
df_y_FFT_dfx15_1 = prepare_transpost_df_x(df_y_concat_100_linhas_1, 1)
df_y_FFT_dfx15_2 = prepare_transpost_df_x(df_y_concat_100_linhas_2, 2)
df_y_FFT_dfx15_3 = prepare_transpost_df_x(df_y_concat_100_linhas_3, 3)
df_y_FFT_dfx15_4 = prepare_transpost_df_x(df_y_concat_100_linhas_4, 4)

df_z_FFT_dfx15_0 = prepare_transpost_df_x(df_z_concat_100_linhas_0, 0)
df_z_FFT_dfx15_1 = prepare_transpost_df_x(df_z_concat_100_linhas_1, 1)
df_z_FFT_dfx15_2 = prepare_transpost_df_x(df_z_concat_100_linhas_2, 2)
df_z_FFT_dfx15_3 = prepare_transpost_df_x(df_z_concat_100_linhas_3, 3)
df_z_FFT_dfx15_4 = prepare_transpost_df_x(df_z_concat_100_linhas_4, 4)


####################################################################################

### PLOT FOR VISUALIZING DATA IN THE FREQUENCY DOMAIN, PRESENTS ONLY ONE ACQUISITION OF EACH LABEL

####################################################################################


### PLOT FOR VISUALIZING DATA IN THE TIME DOMAIN, PRESENTS ONLY ONE ACQUISITION OF EACH LABEL

figx, axs = plt.subplots(5, 1, figsize=(30, 10))

# Gráfico 1
axs[0].bar(range(len(df_x_FFT_dfx15_0['Dado_x15_0_0'])), df_x_FFT_dfx15_0['Dado_x15_0_0'])
axs[0].set_xlim([0, 10000])

axs[1].bar(range(len(df_x_FFT_dfx15_1['Dado_x15_0_1'])), df_x_FFT_dfx15_1['Dado_x15_0_1'])
axs[1].set_xlim([0, 10000])

axs[2].bar(range(len(df_x_FFT_dfx15_2['Dado_x15_0_2'])), df_x_FFT_dfx15_2['Dado_x15_0_2'])
axs[2].set_xlim([0, 10000])

axs[3].bar(range(len(df_x_FFT_dfx15_3['Dado_x15_0_3'])), df_x_FFT_dfx15_3['Dado_x15_0_3'])
axs[3].set_xlim([0, 10000])

axs[4].bar(range(len(df_x_FFT_dfx15_4['Dado_x15_0_4'])), df_x_FFT_dfx15_4['Dado_x15_0_4'])
axs[4].set_xlim([0, 10000])

# Títulos e rótulos
axs[0].set(title='Frequence Domain - New Dataset')
axs[0].set(ylabel='Label 0 \n\n')
axs[1].set(ylabel='Label 1 \n\n')
axs[2].set(ylabel='Label 2 \n\nValue')
axs[3].set(ylabel='Label 3 \n\n')
axs[4].set(xlabel='Frequence')
axs[4].set(ylabel='Label 4 \n\n')

# Ajustar tamanhos
axs[0].figure.set_size_inches(20, 10)

# Ocultar eixos x, exceto o último
axs[0].axes.get_xaxis().set_visible(False)
axs[1].axes.get_xaxis().set_visible(False)
axs[2].axes.get_xaxis().set_visible(False)
axs[3].axes.get_xaxis().set_visible(False)

# Grades
axs[0].grid()
axs[1].grid()
axs[2].grid()
axs[3].grid()
axs[4].grid()

# Salvar e mostrar
string = r'images\frequenceplot.png'
plt.savefig(string)
plt.show()
plt.cla()
plt.clf()
plt.close()


### CONCATENES THE DATASETS INTO A SINGLE DATASET, ALL FAULTS IN THE SAME DATASET, USING ONLY THE X AXIS



df_list = [
    df_x_FFT_dfx15_0,
    df_x_FFT_dfx15_1,
    df_x_FFT_dfx15_2,
    df_x_FFT_dfx15_3,
    df_x_FFT_dfx15_4
]

df_total_X = pd.concat(df_list, axis=1)


df_total_T_X = df_total_X.T


df_list = [
    df_y_FFT_dfx15_0,
    df_y_FFT_dfx15_1,
    df_y_FFT_dfx15_2,
    df_y_FFT_dfx15_3,
    df_y_FFT_dfx15_4
]


df_total_Y = pd.concat(df_list, axis=1)


df_total_T_Y = df_total_Y.T



df_list = [
    df_z_FFT_dfx15_0,
    df_z_FFT_dfx15_1,
    df_z_FFT_dfx15_2,
    df_z_FFT_dfx15_3,
    df_z_FFT_dfx15_4
]

df_total_Z = pd.concat(df_list, axis=1)


df_total_T_Z = df_total_Z.T




df_list = [
    df_total_T_X,
    df_total_T_Y,
    df_total_T_Z
]

RNA_Final_FFT = pd.concat(df_list, axis=1)



labels = []

for i in range(5):
    for j in range(int(len(df_total_T_X)/5)):
        labels.append(i)
  



#################################################################
##### KMEANS
#################################################################


### TRAINING THE KMEANS ALGORITHM ###

clusters = 5

kmeans = KMeans(n_clusters=clusters)

kmeans.fit(RNA_Final_FFT)

y_kmeans = kmeans.predict(RNA_Final_FFT)


### IT IS NECESSARY TO MANUALLY CHECK WHICH NUMBER CORRESPOND TO WHICH TYPE OF DEFECT
### HOW DO WE KNOW THE LABELS AND POSSIBLE TO DO AN ACCURACY TEST TO CHECK HOW KMEAS PERFORMED
 
labels = []

for i in range(5):
    for j in range(int(len(df_total_T_X)/5)):
        labels.append(i)
  
 
  
df_acurracy_f = []
df_acurracy_f = pd.DataFrame(df_acurracy_f)
df_acurracy_f['K-means'] = y_kmeans
df_acurracy_f['Real'] = labels

df_acurracy_f['Real'].replace(1, 'operating condition 0',inplace=True)
df_acurracy_f['Real'].replace(4, 'operating condition 1',inplace=True)
df_acurracy_f['Real'].replace(2, 'operating condition 2',inplace=True)
df_acurracy_f['Real'].replace(0, 'operating condition 3',inplace=True)
df_acurracy_f['Real'].replace(3, 'operating condition 4',inplace=True)

for i in range(5):
    lista_valores = df_acurracy_f['K-means'][i*99:(i+1)*99]
    contagem = Counter(lista_valores)
    mais_comum = contagem.most_common(5)
    print(mais_comum)
        
df_acurracy_f['K-means'].replace(4, 'operating condition 0',inplace=True)
df_acurracy_f['K-means'].replace(3, 'operating condition 1',inplace=True)
df_acurracy_f['K-means'].replace(2, 'operating condition 2',inplace=True)
df_acurracy_f['K-means'].replace(1, 'operating condition 3',inplace=True)
df_acurracy_f['K-means'].replace(0, 'operating condition 4',inplace=True)


from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

target_names = ['op. cond. 0','op. cond. 1', 'op. cond. 2', 'op. cond. 3', 'op. cond. 4']

from sklearn.metrics import confusion_matrix
fig = plt.figure(figsize=(13,13))
mat = confusion_matrix(df_acurracy_f['Real'].values, df_acurracy_f['K-means'].values)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap="Blues",
            xticklabels=target_names, yticklabels=target_names,
            annot_kws={"size": 26},  # Tamanho das anotações no heatmap
            )
plt.xlabel('True label', fontsize=22)  # Tamanho do rótulo do eixo X
plt.ylabel('Predicted label', fontsize=22)  # Tamanho do rótulo do eixo Y
plt.xticks(fontsize=17)  # Tamanho dos ticks do eixo X
plt.yticks(fontsize=17)  # Tamanho dos ticks do eixo Y
string = r'images\KMeans_Apenas_FFT.png'
plt.savefig(string)
plt.show()
plt.cla()   # Clear axis
plt.clf()   # Clear figure
plt.close() # Close a figure window


from sklearn.metrics import classification_report

print(classification_report(df_acurracy_f['K-means'].values, df_acurracy_f['Real'].values, target_names=target_names))







#################################################################
##### ARTIFICIAL NEURAL NETWORK
#################################################################




#################################################################
##### Primeiro verificar a quantidade de neurônios e arquitetura ideal
#################################################################

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.models import Sequential, load_model
import serial
import time
from sklearn.metrics import accuracy_score, classification_report
import os
from scipy.fft import fft, fftfreq

from sklearn.model_selection import StratifiedKFold



def find_best_nn(X_train, y_train, X_val, y_val, max_neurons, step=1):
    """
    Encontra o melhor número de neurônios para uma rede neural usando validação simples.

    Parâmetros:
        X_train (np.array): Dados de entrada de treino.
        y_train (np.array): Rótulos categóricos (one-hot encoded) de treino.
        X_val (np.array): Dados de entrada de validação.
        y_val (np.array): Rótulos categóricos (one-hot encoded) de validação.
        max_neurons (int): Número máximo de iterações (multiplicador de neurônios).
        step (int): Incremento no número de neurônios.

    Retorna:
        tuple: Lista de métricas e configuração do melhor modelo baseado na acurácia.
    """
    metricas_pca = []

    # Função para criar o modelo
    def create_model(qnt_neu):
        model = Sequential()
        model.add(Dense(qnt_neu, input_dim=X_train.shape[1], kernel_initializer='normal', activation='sigmoid'))
        #model.add(Dropout(0.2))
        #model.add(Dense(2 * qnt_neu, activation='sigmoid'))
        #model.add(Dropout(0.2))
        #model.add(Dense(2 * qnt_neu, activation='sigmoid'))
        model.add(Dense(y_train.shape[1], kernel_initializer='normal', activation='softmax'))
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        return model

    # Testar diferentes números de neurônios
    for i in range(1, max_neurons, step):
        qnt_neu = 10 * i
        print(f"Quantidade de neurônios: {qnt_neu}")

        # Criar e treinar o modelo
        model = create_model(qnt_neu)
        history = model.fit(
            X_train, y_train,
            batch_size=10,
            epochs=100,
            verbose=0,
            validation_data=(X_val, y_val)
        )

        # Avaliar o modelo
        scores = model.evaluate(X_val, y_val, verbose=0)
        metricas_pca.append({
            'qnt_neuronios': qnt_neu,
            'loss': scores[0],
            'accuracy': scores[1]
        })

        print(f"Neurônios: {qnt_neu} | Loss: {scores[0]:.4f} | Accuracy: {scores[1]:.4f}")

    # Selecionar o melhor modelo baseado na acurácia
    melhor_metodo = max(metricas_pca, key=lambda x: x['accuracy'])
    return metricas_pca, melhor_metodo



def preprocess_data_simple(df, num_classes=5, test_size=0.2, random_state=42):
    """
    Preprocessa os dados para treinamento de uma rede neural, incluindo:
    - Encoding do rótulo.
    - Padronização das features.
    - Divisão dos dados em treino e teste.
    - Conversão dos rótulos para formato categórico (one-hot encoding).

    Parâmetros:
        df (pd.DataFrame): DataFrame com os dados.
        target_column (str): Nome da coluna que contém os rótulos.
        num_classes (int): Número de classes para o one-hot encoding.
        test_size (float): Proporção dos dados para o conjunto de teste.
        random_state (int): Semente para reprodução dos resultados.

    Retorna:
        tuple: Contendo X_train, X_test, y_train, y_test.
    """
    # Inicializar o LabelEncoder
    label_encoder = LabelEncoder()
    lista_rotulos_numerico = label_encoder.fit_transform(df['classe'].values)

    # Remover a coluna de rótulos do DataFrame
    df = df.drop(columns=['classe'])


    df_norm = StandardScaler().fit_transform(df)

    # Definir X e y
    X = df_norm
    y = lista_rotulos_numerico

    # Dividir os dados em treino e teste
    X_train, X_test, y_train_not_categorical, y_test_not_categorical = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )

    X_train = X_train
    X_test = X_test
    
    # Converter rótulos para formato categórico (one-hot encoding)
    y_train = to_categorical(y_train_not_categorical, num_classes=num_classes)
    y_test = to_categorical(y_test_not_categorical, num_classes=num_classes)

    return X_train, X_test, y_train, y_test, y_train_not_categorical, y_test_not_categorical



RNA_Final_FFT['classe'] = labels

X_train, X_test, y_train, y_test, y_train_not_categorical, y_test_not_categorical = preprocess_data_simple(RNA_Final_FFT)

max_neurons = 20
metricas, melhor_modelo = find_best_nn(X_train, y_train, X_test, y_test, max_neurons, step=1)





########################################################################
########################################################################
########################################################################


import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import ParameterGrid
from tensorflow.keras.callbacks import EarlyStopping
from joblib import Parallel, delayed


RNA_Final_FFT = RNA_Final_FFT.drop(columns=['classe'])


X = RNA_Final_FFT
y = labels

# Normalizar os dados
scaler = StandardScaler()
X = scaler.fit_transform(X)

# One-hot encoding para as classes
y = to_categorical(y)

qnt_neurons = 10

print(f"============ NEURONIOS {qnt_neurons}  ==============")

# Definir a função para criar o modelo
def create_model(optimizer, init, activation):
    model = Sequential()
    model.add(Input(shape=(X.shape[1],)))
    model.add(Dense(qnt_neurons, kernel_initializer=init, activation=activation))
    model.add(Dense(y.shape[1], activation='softmax'))
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Configurar hiperparâmetros para busca
param_grid = {
    'batch_size': [1, 5, 10, 20],
    'epochs': [10, 50, 100, 200, 300],
    'model__optimizer': ['adam', 'rmsprop', 'SGD'],
    'model__init': ['uniform', 'normal'],
    'model__activation': ['relu', 'tanh', 'sigmoid', 'linear']
}

kf = KFold(n_splits=3, shuffle=True, random_state=42)
param_combinations = list(ParameterGrid(param_grid))
best_accuracy = 0
best_params = None

resultados = []

start_time = time.time()

# Função para processar uma combinação de hiperparâmetros
def process_combination(params):
    accuracies = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Criar e treinar o modelo
        model = create_model(
            optimizer=params['model__optimizer'],
            init=params['model__init'],
            activation=params['model__activation']
        )

        # Configurar EarlyStopping
        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

        # Treinamento do modelo
        model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=params['epochs'],
            batch_size=params['batch_size'],
            callbacks=[early_stopping],
            verbose=0
        )

        # Avaliar o modelo
        y_pred = np.argmax(model.predict(X_test), axis=1)
        y_test_labels = np.argmax(y_test, axis=1)
        accuracy = accuracy_score(y_test_labels, y_pred)
        accuracies.append(accuracy)

    mean_accuracy = np.mean(accuracies)
    return params, mean_accuracy

# Paralelizar o processo de validação cruzada
results = Parallel(n_jobs=-1)(delayed(process_combination)(params) for params in param_combinations)

# Identificar os melhores hiperparâmetros
for params, mean_accuracy in results:
    print(f'Hiperparâmetros: {params} - Acurácia Média: {mean_accuracy:.4f}')
    if mean_accuracy > best_accuracy:
        best_accuracy = mean_accuracy
        best_params = params

resultados.append({
'qnt_neurons': qnt_neurons,
'parametros': best_params,
'melhores_parametros': best_accuracy
})
# Exibir os melhores hiperparâmetros e resultados
print(f'Melhores hiperparâmetros: {best_params}')
print(f'Melhor acurácia média: {best_accuracy:.4f}')


### HIPERPARAMETRIZATION WITH 1 LAYER
#Melhores hiperparâmetros: {'batch_size': 1, 'epochs': 10, 'model__activation': 'relu', 'model__init': 'uniform', 'model__optimizer': 'rmsprop'}


#Melhor acurácia média: 1.0000
# Tempo de execução: 7871.282785892487 segundos

end_time = time.time()

execution_time_hiperparametrization = end_time - start_time

# Exibir os resultados
print(f"Tempo de início: {start_time}")
print(f"Tempo de fim: {end_time}")
print(f"Tempo de execução: {execution_time_hiperparametrization} segundos")


################################################################################################
################################################################################################
################################################################################################

# Preparação dos dados
X = RNA_Final_FFT
y = labels

# Normalizar os dados
scaler = StandardScaler()
X = scaler.fit_transform(X)

# One-hot encoding para as classes
y = to_categorical(y)

# Dividir os dados em treinamento+validação (70%) e teste (30%)
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=labels)

# Parâmetros fixos
qnt_neurons = 10
fixed_params = {
    'batch_size': 1,
    'epochs': 50,
    'optimizer': 'rmsprop',
    'init': 'uniform',
    'activation': 'relu'
}

print(f"============ NEURONIOS {qnt_neurons}  ==============")

# Função para criar o modelo
def create_model():
    model = Sequential()
    model.add(Input(shape=(X_train_val.shape[1],)))
    model.add(Dense(qnt_neurons, kernel_initializer=fixed_params['init'], activation=fixed_params['activation']))
    model.add(Dense(y.shape[1], activation='softmax'))
    model.compile(optimizer=fixed_params['optimizer'], loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Divisão do conjunto de treinamento+validação para validação cruzada
kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []

# Para armazenar as curvas de perda e acurácia de todos os folds
losses = []
val_losses = []
accuracies = []
val_accuracies = []

start_time = time.time()

for train_index, val_index in kf.split(X_train_val):
    X_train, X_val = X_train_val[train_index], X_train_val[val_index]
    y_train, y_val = y_train_val[train_index], y_train_val[val_index]
    
    # Criar e treinar o modelo
    model = create_model()
    
    # Treinamento do modelo
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=fixed_params['epochs'],
        batch_size=fixed_params['batch_size'],
        verbose=0
    )
    
    # Salvar as métricas do histórico
    losses.append(history.history['loss'])
    val_losses.append(history.history['val_loss'])
    accuracies.append(history.history['accuracy'])
    val_accuracies.append(history.history['val_accuracy'])
    
    # Avaliação no conjunto de validação
    y_val_pred = np.argmax(model.predict(X_val), axis=1)
    y_val_labels = np.argmax(y_val, axis=1)
    accuracy = accuracy_score(y_val_labels, y_val_pred)
    fold_accuracies.append(accuracy)


end_time = time.time()

execution_time_train = end_time - start_time

# Exibir os resultados
print(f"Tempo de início: {start_time}")
print(f"Tempo de fim: {end_time}")
print(f"Tempo de execução: {execution_time_train} segundos")


# Calcular as médias sem necessidade de truncamento
mean_loss = np.mean(losses, axis=0)
mean_val_loss = np.mean(val_losses, axis=0)
mean_accuracy = np.mean(accuracies, axis=0)
mean_val_accuracy = np.mean(val_accuracies, axis=0)

# Plotar as curvas de perda e acurácia médias
plt.figure(figsize=(12, 5))

# Curva de perda
plt.subplot(1, 2, 1)
plt.plot(mean_loss[:10], label='Train')
plt.plot(mean_val_loss[:10], label='Validation')
plt.title('Loss Curve')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Curva de acurácia
plt.subplot(1, 2, 2)
plt.plot(mean_accuracy[:10], label='Train')
plt.plot(mean_val_accuracy[:10], label='Validation')
plt.title('Accuracy Curve')
plt.xlabel('epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Resultados da validação cruzada
mean_validation_accuracy = np.mean(fold_accuracies)
print(f"Acurácias por dobra (validação): {fold_accuracies}")
print(f"Acurácia média na validação cruzada: {mean_validation_accuracy:.4f}")


start_time = time.time()

# Avaliação no conjunto de teste final
y_test_pred = np.argmax(model.predict(X_test), axis=1)
y_test_labels = np.argmax(y_test, axis=1)
final_test_accuracy = accuracy_score(y_test_labels, y_test_pred)
print(f"Acurácia no conjunto de teste: {final_test_accuracy:.4f}")


end_time = time.time()

execution_time_inference = end_time - start_time

# Exibir os resultados
print(f"Tempo de início: {start_time}")
print(f"Tempo de fim: {end_time}")
print(f"Tempo de execução: {execution_time_inference} segundos")


target_names = ['op. cond. 0', 'op. cond. 1', 'op. cond. 2', 'op. cond. 3', 'op. cond. 4']

from sklearn.metrics import confusion_matrix
fig = plt.figure(figsize=(13,13))
mat = confusion_matrix(y_test_labels, y_test_pred)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap="Blues",
            xticklabels=target_names, yticklabels=target_names,
            annot_kws={"size": 26},  # Tamanho das anotações no heatmap
            )
plt.xlabel('True label', fontsize=22)  # Tamanho do rótulo do eixo X
plt.ylabel('Predicted label', fontsize=22)  # Tamanho do rótulo do eixo Y
plt.xticks(fontsize=17)  # Tamanho dos ticks do eixo X
plt.yticks(fontsize=17)  # Tamanho dos ticks do eixo Y
string = r'images\ANN_FFT.png'
plt.savefig(string)
plt.show()
plt.cla()   # Clear axis
plt.clf()   # Clear figure
plt.close() # Close a figure window


from sklearn.metrics import classification_report

print(classification_report(y_test_labels, y_test_pred, target_names=target_names))